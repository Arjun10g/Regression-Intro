<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Week 1</title>
    <link rel="stylesheet" href="week1.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script>
        MathJax.Hub.Config({
            extensions: ["tex2jax.js"],
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [['\\(', '\\)']],
                displayMath: [['\\[', '\\]']],
                processEscapes: true
            },
            "HTML-CSS": { scale: 90 },
        });
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.11.5/gsap.min.js" integrity="sha512-cOH8ndwGgPo+K7pTvMrqYbmI8u8k6Sho3js0gOqVWTmQMlLIi6TbqGWRTpf1ga8ci9H3iPsvDLr4X7xwhC/+DQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.11.5/ScrollTrigger.min.js" integrity="sha512-AMl4wfwAmDM1lsQvVBBRHYENn1FR8cfOTpt8QVbb/P55mYOdahHD4LmHM1W55pNe3j/3od8ELzPf/8eNkkjISQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.12.2/Observer.min.js" integrity="sha512-7xTD1meeGGoAzwZKA0Z8YelV3qAvRltuwACWXpnxtneF7VAMztOTAi3t4laVSaE4Znq4LMPeGUIYWEvKEk5r3Q==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.12.2/Draggable.min.js" integrity="sha512-S6SXKUZ11xkCoD/UuhdXG4B4iiCXng+xW2KCx0lgfQqmdqtjqGgm4WChdYIhO1F/CmH21vnkSUvPEgXNgDwkjg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <script src="https://cdn.jsdelivr.net/npm/gsap@3.12.5/dist/Flip.min.js"></script>
    <script src="week1.js" defer></script>
</head>
<body>
  <div class="nav-bar">
    <div class="logo"><h1>A/G</h1></div>
    <div class="sections">
        <div class="sections-sub">
            <a href="#sec1">Introduction</a>
        </div>
        <div class="sections-sub">
            <a href="#sec2">Significance Testing</a>
        </div>
        <div class="sections-sub">
            <a href="#sec3">Model Fit</a>
        </div>
        <div class="sections-sub">
            <a href="#sec4">Properties and Assumptions</a>
        </div>
    </div>
</div>

<!-- Introduction -->
<div class="intro">
    <div class="headers">
        <h1 class="name">Arjun Ghumman</h1>
        <h1 class="title">Introduction to Simple Linear Regression</h1>
    </div>
</div>

<!-- Section 1 -->
<div class="section sec1">
    <h2 id="sec1">Introduction to Regression</h2>
    <p>Simple linear regression lives up to its name: it is a very straightforward approach for predicting a quantitative response Y on the basis of a single predictor variable X. It assumes that there is approximately a linear relationship between X and Y. But, one often wonders what does that mean?</p>
    <p>It means that the mean of the response variable Y is a linear function of the predictor X (A straight line relationship where as X increases Y increases). Mathematically, we can write this linear relationship as:</p>
    <p>\[Y = \beta_0 + \beta_1X + \varepsilon\]</p>
    <p>
        Where:
    </p>
    <ul>
        <li>\(\beta_0\) (Intercept): The value of \(Y\) when \(X = 0\), representing the starting point of the regression line.</li>
        <li>\(\beta_1\) (Slope): The average change in \(Y\) for a one-unit increase in \(X\), representing the steepness of the line.</li>
        <li>\(\varepsilon\) (Error Term): The difference between the observed \(Y\) and the predicted \(Y\), capturing unexplained variability.</li>
    </ul>

    <div class="image">
        <img src="week2_1.png" alt="">
    </div>

    <div class="image">
        <img src="animated_plot.gif" alt="">
    </div>

    <h3>Objective</h3>
    <p>
        The primary goal of simple linear regression is to estimate the coefficients \(\beta_0\) and \(\beta_1\) in order to minimize the error term \(\varepsilon\). 
        This is achieved using the method of least squares, which minimizes the sum of squared residuals (\(\text{SSR}\)): <span class="red">Simply put - </span> we want to find the line that best fits the data.
    </p>
    <p>\[\text{SSR} = \sum_{i=1}^n (Y_i - (\beta_0 + \beta_1X_i))^2\]</p>
    <p>
        By minimizing \(\text{SSR}\), we find the values of \(\beta_0\) and \(\beta_1\) that best fit the data.
    </p>
    <table border="1" style="width:100%; text-align:left; border-collapse:collapse;">
        <tr>
            <th>Component</th>
            <th>Description</th>
            <th>Purpose</th>
        </tr>
        <tr>
            <td>\(Y_i\)</td>
            <td>Actual observed value of \(Y\) for the \(i\)-th data point.</td>
            <td>This is the "truth" or real-world data we are trying to predict.</td>
        </tr>
        <tr>
            <td>\(\beta_0\)</td>
            <td>The intercept of the regression line.</td>
            <td>Starting value of \(Y\) when \(X = 0\).</td>
        </tr>
        <tr>
            <td>\(\beta_1\)</td>
            <td>The slope of the regression line.</td>
            <td>Defines how much \(Y\) changes for a one-unit increase in \(X\).</td>
        </tr>
        <tr>
            <td>\(X_i\)</td>
            <td>The predictor or independent variable for the \(i\)-th data point.</td>
            <td>Helps predict \(Y_i\) using the regression equation.</td>
        </tr>
        <tr>
            <td>\(Y_i - (\beta_0 + \beta_1X_i)\)</td>
            <td>The residual (difference) between the actual value and the predicted value.</td>
            <td>Measures how far off the prediction is from the actual value.</td>
        </tr>
        <tr>
            <td>\((Y_i - (\beta_0 + \beta_1X_i))^2\)</td>
            <td>The squared residual for the \(i\)-th data point.</td>
            <td>Ensures all errors are positive and emphasizes larger errors.</td>
        </tr>
        <tr>
            <td>\(\sum_{i=1}^n\)</td>
            <td>Summation over all data points.</td>
            <td>Adds up all the squared residuals to get the total error.</td>
        </tr>
    </table>

    <h3>Estimating Coefficients</h3>
    <p>
        The coefficients \(\beta_0\) and \(\beta_1\) are calculated using the following formulas:
    </p>
    <p>\[\beta_1 = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2}\]</p>
    <p>\[\beta_0 = \bar{Y} - \beta_1\bar{X}\]</p>
    <p>
        Where:
    </p>
    <ul>
        <li>\(\bar{X}\) and \(\bar{Y}\) are the means of \(X\) and \(Y\), respectively.</li>
        <li>\(\beta_1\) is the slope, determined by the covariance between \(X\) and \(Y\) divided by the variance of \(X\).</li>
        <li>\(\beta_0\) is the intercept, centering the regression line based on the data.</li>
    </ul>
    <p> The formula for \(\beta_1\), the slope of the regression line, closely resembles the formula for Pearson's correlation coefficient (\(r\)). Specifically: </p>
    <p>\[\beta_1 = \frac{\text{Cov}(X, Y)}{\text{Var}(X)} = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2}\]</p>
    <p> On the other hand, the formula for correlation (\(r\)) is given as: </p>
    <p>\[r = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X) \cdot \text{Var}(Y)}}\]</p>
    <p> The key difference between the two lies in the denominator: </p>
         <ul> 
            <li>\(\beta_1\) scales the covariance by the variance of \(X\) alone, as it represents the change in \(Y\) for a one-unit change in \(X\).</li>
            <li>\(r\), on the other hand, normalizes the covariance by the standard deviations of both \(X\) and \(Y\), making it a unitless measure of the strength and direction of the relationship between \(X\) and \(Y\).</li>
         </ul> 
    <p>In essence, <span class="red">Correlation is the standardized version of a slope.</span></p>
    <h3>Residuals</h3>
    <div class="image">
        <img src="week2_2.png" alt="">
    </div>
    <p>
        Residuals are the differences between the observed values of the response variable \(Y\) and the values predicted by the regression line. They are calculated as:
    </p>
    <p>\[e_i = Y_i - (\beta_0 + \beta_1X_i)\]</p>

    <p>
        Residuals are useful for assessing how well the model fits. A good model will have residuals that are randomly distributed around zero, indicating that the model captures the underlying relationship between the variables. 
        On the other hand, patterns in the residuals (e.g., a curved relationship or heteroscedasticity) suggest that the model may not be appropriate for the data.
    </p>
</div>
      

<!-- Section 2 -->

<div class="section sec2">
    <h2 id="sec2">Significance Testing and Measures of Uncertainty</h2>

    <h3>Standard Errors</h3>
    <p>
        Standard errors measure the uncertainty in the estimated coefficients (\(\beta_0\) and \(\beta_1\)). These are calculated as:
    </p>
    <p>\[ SE(\beta_0) = \sqrt{\frac{\sigma^2}{n} + \frac{\bar{X}^2}{\sum_{i=1}^n (X_i - \bar{X})^2}} \]</p>
    <p>\[ SE(\beta_1) = \sqrt{\frac{\sigma^2}{\sum_{i=1}^n (X_i - \bar{X})^2}} \]</p>
    <p>Where:</p>
    <ul>
        <li>\(\sigma^2\) is the variance of the error term (\(\varepsilon\)), estimated using the residuals.</li>
        <li>\(n\) is the sample size, representing the number of data points.</li>
        <li>\(\bar{X}\) is the mean of the predictor variable \(X\).</li>
        <li>\(X_i\) represents each individual value of \(X\).</li>
    </ul>
    <ul>
        <li>Smaller standard errors indicate greater precision in the coefficient estimates.</li>
        <li>Standard errors decrease as the sample size (\(n\)) increases.</li>
    </ul>

    <h3>Breaking It Down</h3>
    <p>Here is how these formulas work in a simplified way:</p>
    <ul>
        <li>
            To estimate \(\sigma^2\), we calculate the residual variance, which measures how far the actual values of \(Y\) are from the predicted values (\(\hat{Y}\)):
            \[
            \sigma^2 = \frac{\text{Residual Sum of Squares (RSS)}}{n - 2}
            \]
            Where:
            <ul>
                <li>\(\text{RSS} = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2\)</li>
                <li>\(n - 2\) accounts for the two parameters (\(\beta_0\) and \(\beta_1\)) estimated in the model.</li>
            </ul>
        </li>
        <li>
            Once \(\sigma^2\) is estimated, we plug it into the formulas for \(SE(\beta_0)\) and \(SE(\beta_1)\) to calculate the variability of the coefficients:
            <ul>
                <li>\(SE(\beta_1)\): Divides \(\sigma^2\) by the variance in \(X\). More spread in \(X\) gives us more "leverage" to estimate the slope precisely.</li>
                <li>\(SE(\beta_0)\): Combines \(\sigma^2\) with an additional term accounting for the average value of \(X\), since the intercept depends on the position of \(X = 0\).</li>
            </ul>
        </li>
    </ul>

    <h3>Hypothesis Testing</h3>
    <p>
        To test whether there is a relationship between \(X\) and \(Y\), we evaluate the null hypothesis (\(H_0\)) and alternative hypothesis (\(H_a\)):
    </p>
    <p>\[ H_0: \beta_1 = 0 \quad \text{vs.} \quad H_a: \beta_1 \neq 0 \]</p>
    <ul>
        <li>\(H_0\): No relationship exists between \(X\) and \(Y\).</li>
        <li>\(H_a\): A relationship exists between \(X\) and \(Y\).</li>
    </ul>
    <p>
        We calculate the <strong>t-statistic</strong> to determine if \(\beta_1\) is significantly different from 0:
    </p>
    <p>\[ t = \frac{\beta_1 - 0}{SE(\beta_1)} \]</p>
    <p>
        The t-statistic measures how many standard deviations \(\beta_1\) is away from 0. Using the t-distribution, we compute the <strong>p-value</strong>, which indicates the probability of observing such a result if \(H_0\) is true.
    </p>
    <p>
        A small p-value (e.g., \(p < 0.05\)) suggests rejecting \(H_0\), supporting the conclusion that \(X\) and \(Y\) are related.
    </p>

    <h3>Confidence Intervals</h3>
    <p>
        Confidence intervals provide a range of plausible values for the true coefficient. For example, a 95% confidence interval for \(\beta_1\) is calculated as:
    </p>
    <p>\[ \beta_1 \pm 2 \cdot SE(\beta_1) \]</p>
    <p>
        This means that if we repeated the analysis many times, 95% of the intervals would contain the true value of \(\beta_1\).
    </p>
</div>


<div class="section sec3">
    <h2 id="sec3">Model Fit</h2>

    <h3>Overview</h3>
    <p>
        To assess how well the model fits the data. This is typically done using two related metrics:
    </p>
    <ul>
        <li><strong>Residual Standard Error (RSE)</strong></li>
        <li><strong>\(R^2\) Statistic</strong></li>
    </ul>
    <p>
        These metrics provide insight into the accuracy of the model and the proportion of variability in the response variable that is explained by the predictor.
    </p>

    <h3>Residual Standard Error (RSE)</h3>
    <p>
        The RSE measures the average distance between the observed data points and the regression line. It is calculated as:
    </p>
    <p>\[\text{RSE} = \sqrt{\frac{\sum_{i=1}^n (Y_i - \hat{Y}_i)^2}{n-2}}\]</p>
    <ul>
        <li><strong>\(Y_i\):</strong> The actual observed value of \(Y\).</li>
        <li><strong>\(\hat{Y}_i\):</strong> The predicted value of \(Y\) from the regression model.</li>
        <li><strong>\(n\):</strong> The total number of data points.</li>
    </ul>
    <p>
        <span class="red">RSE</span> simply tells us how far, on average, the actual data points are from the predicted values. A smaller RSE means the model fits the data better.
    </p>

    <h3>\(R^2\) Statistic</h3>
    <p>
        The \(R^2\) statistic measures the proportion of variability in the response variable \(Y\) that is explained by the predictor \(X\). It is calculated as:
    </p>
    <p>\[R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}\]</p>
    <ul>
        <li><strong>\(\text{RSS}\):</strong> Residual Sum of Squares, the variability left unexplained by the model.</li>
        <li><strong>\(\text{TSS}\):</strong> Total Sum of Squares, the total variability in \(Y\).</li>
    </ul>
    <p>
        \(R^2\) ranges between 0 and 1:
        <ul>
            <li>A value close to 1 means the model explains most of the variability in \(Y\).</li>
            <li>A value close to 0 means the model explains little of the variability in \(Y\).</li>
        </ul>
    </p>
    <p>
        <span class="red">\(R^2\)</span> shows how much of the "story" about \(Y\) is explained by \(X\). The closer \(R^2\) is to 1, the better the fit.
    </p>

    <h3>Comparison</h3>
    <p>
        <strong>Residual Standard Error:</strong> Provides an absolute measure of the model's fit, in the units of \(Y\). This can be thought of as a measure of lack of fit, then a measure of fit.
    </p>
    <p>
        <strong>\(R^2\):</strong> Provides a relative measure of fit, showing the proportion of explained variance, independent of the units of \(Y\).
    </p>

    <h3>Example</h3>
    <p>
        Let's say for an example we are trying to predict the sales of TV sets based on the advertising budget:
    </p>
    <table border="1" style="width:100%; text-align:left; border-collapse:collapse;">
        <tr>
            <th>Metric</th>
            <th>Value</th>
            <th>Interpretation</th>
        </tr>
        <tr>
            <td>Residual Standard Error</td>
            <td>3.26</td>
            <td>This value would suggest that average deviation of actual sales from the predicted sales is about 3,260 units.</td>
        </tr>
        <tr>
            <td>\(R^2\) Statistic</td>
            <td>0.612</td>
            <td>About 61% of the variability in sales is explained by TV advertising.</td>
        </tr>
    </table>
    <h3>Simplifying R Squared - </h3>
    <p>
        The \(R^2\) statistic is a measure that tells us <strong>how much of the changes or variability in the response variable (\(Y\)) can be explained by the predictor variable (\(X\))</strong> using the regression model.
    </p>

    <h3>Breaking it Down:</h3>
    <ul>
        <li>
            <strong>What does "variability in \(Y\)" mean?</strong>
            <ul>
                <li>Variability in \(Y\) refers to how much the values of \(Y\) differ from each other. For example, if you have a dataset of student test scores, some scores will be high, and some will be low. The variability is the range and spread of these scores.</li>
                <li>If there is no variability in \(Y\), all the values would be the same (e.g., every student scored exactly 70). But in real-world data, variability always exists.</li>
            </ul>
        </li>
        <li>
            <strong>What does \(R^2\) measure?</strong>
            <ul>
                <li>\(R^2\) tells us how much of that variability in \(Y\) can be linked to or predicted by changes in \(X\). For example:
                    <ul>
                        <li>If \(R^2 = 0.75\), it means 75% of the variability in \(Y\) is explained by \(X\), and the remaining 25% is due to other factors not included in the model or random noise.</li>
                        <li>If \(R^2 = 0.25\), only 25% of the variability in \(Y\) is explained by \(X\), and 75% is unexplained.</li>
                    </ul>
                </li>
            </ul>
        </li>
        <li>
            <strong>How is \(R^2\) calculated?</strong>
            <ul>
                <li>It's based on the <em>proportion of total variability</em> that the regression model accounts for. Mathematically:
                    <div class="equation">
                        \[ R^2 = 1 - \frac{\text{Unexplained Variability (RSS)}}{\text{Total Variability (TSS)}} \]
                    </div>
                </li>
                <li><strong>RSS</strong> (Residual Sum of Squares): How much of \(Y\)'s variability is left unexplained by the model.</li>
                <li><strong>TSS</strong> (Total Sum of Squares): The total variability in \(Y\).</li>
            </ul>
        </li>
        <li>
            <strong>Why does \(R^2\) matter?</strong>
            <ul>
                <li><strong>High \(R^2\):</strong> The model does a good job explaining or predicting \(Y\). For example, if \(R^2 = 0.9\), it means most of \(Y\)'s variability is captured by the regression model.</li>
                <li><strong>Low \(R^2\):</strong> The model doesn’t explain much about \(Y\), meaning other factors or a different model might be needed.</li>
            </ul>
        </li>
    </ul>

</div>

<div class="section sec4">
    <h2 id="sec4">Properties and Assumptions</h2>
    <h3>General Properties of a Linear Model</h3>
    <p>
        A linear regression model relies on several key assumptions and properties to ensure its validity. Below is a table summarizing these properties and what they mean:
    </p>

    <table border="1" style="width:100%; text-align:left; border-collapse:collapse;">
        <tr>
            <th>Property</th>
            <th>Description</th>
            <th>What It Means</th>
        </tr>
        <tr>
            <td>Linearity</td>
            <td>The relationship between the predictor (\(X\)) and response (\(Y\)) is linear.</td>
            <td>For every unit increase in \(X\), there is a constant change in \(Y\). The data should follow a straight-line trend when plotted.</td>
        </tr>
        <tr>
            <td>Monotonicity</td>
            <td>The relationship between \(X\) and \(Y\) is either consistently increasing or decreasing.</td>
            <td>There should be no reversals in the trend (e.g., \(Y\) does not first increase and then decrease as \(X\) increases).</td>
        </tr>
        <tr>
            <td>Homoscedasticity</td>
            <td>The variance of the residuals (errors) is constant across all levels of \(X\).</td>
            <td>The spread of the residuals should remain the same, rather than increasing or decreasing as \(X\) changes.</td>
        </tr>
        <tr>
            <td>Independence</td>
            <td>Observations are independent of each other.</td>
            <td>Errors for one data point should not be related to errors for another.</td>
        </tr>
        <tr>
            <td>Normality</td>
            <td>The residuals (errors) are normally distributed.</td>
            <td>When plotted, the errors should form a bell-shaped curve, ensuring valid hypothesis testing and confidence intervals.</td>
        </tr>
        <tr>
            <td>No Multicollinearity</td>
            <td>The predictor variables (in multiple regression) are not highly correlated with each other.</td>
            <td>High correlation between predictors can distort the estimates of the coefficients and reduce the reliability of the model.</td>
        </tr>
        <tr>
            <td>Exogeneity</td>
            <td>The predictor variable (\(X\)) is uncorrelated with the error term (\(\varepsilon\)).</td>
            <td>\(X\) should not contain measurement errors or be influenced by omitted variables that affect \(Y\).</td>
        </tr>
    </table>

    <h3>Understanding the Problem - </h3>
    <div class="image">
        <img src="https://miro.medium.com/v2/resize:fit:1400/1*WjgaFV1WGPbw3-fR-2CB0A.png" alt="">
    </div>
    <h3>Bias</h3>
    <p>
        Bias refers to the error introduced when the model makes simplifying assumptions about the real-world relationship between \(X\) and \(Y\). In linear regression, bias occurs when the true relationship is different, from what the model assumes. For example, if the true relationship is non-linear, but the model assumes a linear relationship, bias will be introduced.
    </p>
    <p>
        Mathematically, bias is defined as:
    </p>
    <p>\[ \text{Bias}(\hat{f}) = \mathbb{E}[\hat{f}(X)] - f(X) \]</p>
    <p>
        Where:
    </p>
    <ul>
        <li>\(\mathbb{E}[\hat{f}(X)]\): The expected prediction from the model over different datasets.</li>
        <li>\(f(X)\): The true underlying function.</li>
    </ul>
    <p>
        A high bias model will systematically underfit the data, failing to capture the true relationship between \(X\) and \(Y\).
    </p>
    
    <h3>Variance</h3>
    <p>
        Variance measures the model's sensitivity to fluctuations. High variance models capture noise as if it were a true signal. Simply put, variance is the variability in model predictions across different datasets. 
    </p>
    <p>
        Mathematically, variance is defined as:
    </p>
    <p>\[ \text{Variance}(\hat{f}) = \mathbb{E}[(\hat{f}(X) - \mathbb{E}[\hat{f}(X)])^2] \]</p>
    <p>
        Where:
    </p>
    <ul>
        <li>\(\hat{f}(X)\): The model's prediction for a specific dataset.</li>
        <li>\(\mathbb{E}[\hat{f}(X)]\): The expected prediction from the model over all datasets.</li>
    </ul>
    <p>
        High variance models perform well on one data but poorly on new, unseen data. For example, a study on the relationship of IQ and gender for psychology students may not generalize to a broader population.
    </p>

    <h3>Impact of Assumption Violations</h3>

    <div class="image">
        <img src="https://www.jmp.com/en_sg/statistics-knowledge-portal/what-is-regression/simple-linear-regression-assumptions/_jcr_content/par/styledcontainer_2069/par/lightbox_e99c/lightboxImage.img.png/1548351203100.png" alt="">
    </div>

    <div class="image">
        <img src="week2_4.png" alt="">
    </div>

    <div class="image">
        <img src="week2_5.png" alt="">
    </div>

    <table border="1" style="width:100%; text-align:left; border-collapse:collapse;">
        <tr>
            <th>Assumption</th>
            <th>Violation</th>
            <th>Impact on Estimates</th>
            <th>Impact on Accuracy (Standard Errors & p-values)</th>
        </tr>
        <tr>
            <td>Linearity</td>
            <td>Non-linear relationship between predictors and response.</td>
            <td>Estimates are biased because the true relationship is not captured.</td>
            <td>Standard errors are underestimated, leading to misleadingly small p-values.</td>
        </tr>
        <tr>
            <td>Homoscedasticity</td>
            <td>Non-constant variance of residuals.</td>
            <td>Estimates remain unbiased.</td>
            <td>Standard errors are incorrect, leading to invalid confidence intervals and hypothesis tests.</td>
        </tr>
        <tr>
            <td>Independence</td>
            <td>Residuals are correlated (e.g., time-series data).</td>
            <td>Estimates may be biased due to residual dependencies.</td>
            <td>Standard errors are underestimated, inflating Type I error rates (false positives).</td>
        </tr>
        <tr>
            <td>Normality</td>
            <td>Residuals are not normally distributed.</td>
            <td>Estimates remain unbiased.</td>
            <td>p-values and confidence intervals become unreliable for small sample sizes.</td>
        </tr>
        <tr>
            <td>No Multicollinearity</td>
            <td>Predictors are highly correlated.</td>
            <td>Estimates are unstable and unreliable.</td>
            <td>Standard errors inflate, leading to large p-values and reduced statistical power.</td>
        </tr>
        <tr>
            <td>Exogeneity</td>
            <td>Predictors are correlated with the error term.</td>
            <td>Severe bias in estimates due to omitted variable bias.</td>
            <td>Standard errors and p-values are invalid, making inference impossible.</td>
        </tr>
        <tr>
            <td>Outliers</td>
            <td>Extreme values in predictors or response.</td>
            <td>Estimates are heavily influenced and may not represent the majority of the data.</td>
            <td>Standard errors increase, leading to unstable p-values and reduced reliability of results.</td>
        </tr>
    </table>
</div>


</body>
</html>